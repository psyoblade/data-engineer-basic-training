# 5ì¼ì°¨. ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ í”„ë¡œì íŠ¸

> ê°€ìƒì˜ ì›¹ ì‡¼í•‘ëª° LGDE.com ì£¼ìš” ì§€í‘œë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ, ì ‘ì†ì •ë³´, ë§¤ì¶œ ë° ê³ ê°ì •ë³´ ë“±ì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ê¸°ë³¸ ì§€í‘œë¥¼ ìƒì„±í•©ë‹ˆë‹¤

- ëª©ì°¨
  * [1. ğŸ“— ìµœì‹ ë²„ì „ ì—…ë°ì´íŠ¸](#1-ìµœì‹ ë²„ì „-ì—…ë°ì´íŠ¸)
  * [2. ğŸ“— í…Œì´ë¸” ìˆ˜ì§‘ ì‹¤ìŠµ](#2-í…Œì´ë¸”-ìˆ˜ì§‘-ì‹¤ìŠµ)
  * [3. ğŸ“— íŒŒì¼ ìˆ˜ì§‘ ì‹¤ìŠµ](#3-íŒŒì¼-ìˆ˜ì§‘-ì‹¤ìŠµ)
  * [4. ğŸ“— ë…¸íŠ¸ë¶ ì»¨í…Œì´ë„ˆ ê¸°ë™](#4-ë…¸íŠ¸ë¶-ì»¨í…Œì´ë„ˆ-ê¸°ë™)
  * [5. ğŸ“— ìˆ˜ì§‘ëœ ë°ì´í„° íƒìƒ‰](#5-ìˆ˜ì§‘ëœ-ë°ì´í„°-íƒìƒ‰)
  * [6. ğŸ“— ê¸°ë³¸ ì§€í‘œ ìƒì„±](#6-ê¸°ë³¸-ì§€í‘œ-ìƒì„±)
  * [7. ğŸ“˜ ê³ ê¸‰ ì§€í‘œ ìƒì„±](#7-ê³ ê¸‰-ì§€í‘œ-ìƒì„±)
  * [8. ğŸ“— ì§ˆë¬¸ ë° ì»¨í…Œì´ë„ˆ ì¢…ë£Œ](#8-ì§ˆë¬¸-ë°-ì»¨í…Œì´ë„ˆ-ì¢…ë£Œ)

<br>

## 1. ìµœì‹ ë²„ì „ ì—…ë°ì´íŠ¸
> ì›ê²© í„°ë¯¸ë„ì— ì ‘ì†í•˜ì—¬ ê´€ë ¨ ì½”ë“œë¥¼ ìµœì‹  ë²„ì „ìœ¼ë¡œ ë‚´ë ¤ë°›ê³ , ê³¼ê±°ì— ì‹¤í–‰ëœ ì»¨í…Œì´ë„ˆê°€ ì—†ëŠ”ì§€ í™•ì¸í•˜ê³  ì¢…ë£Œí•©ë‹ˆë‹¤

### 1-1. ìµœì‹  ì†ŒìŠ¤ë¥¼ ë‚´ë ¤ ë°›ìŠµë‹ˆë‹¤
```bash
# terminal
cd /home/ubuntu/work/data-engineer-${course}-training
git pull
```

### 1-2. í˜„ì¬ ê¸°ë™ë˜ì–´ ìˆëŠ” ë„ì»¤ ì»¨í…Œì´ë„ˆë¥¼ í™•ì¸í•˜ê³ , ì¢…ë£Œí•©ë‹ˆë‹¤

#### 1-2-1. í˜„ì¬ ê¸°ë™ëœ ì»¨í…Œì´ë„ˆë¥¼ í™•ì¸í•©ë‹ˆë‹¤
```bash
# terminal
docker ps -a
```

#### 1-2-2. ê¸°ë™ëœ ì»¨í…Œì´ë„ˆê°€ ìˆë‹¤ë©´ ê°•ì œ ì¢…ë£Œí•©ë‹ˆë‹¤
```bash
# terminal 
docker rm -f `docker ps -aq`
```
> ë‹¤ì‹œ `docker ps -a` ëª…ë ¹ìœ¼ë¡œ ê²°ê³¼ê°€ ì—†ë‹¤ë©´ ëª¨ë“  ì»¨í…Œì´ë„ˆê°€ ì¢…ë£Œë˜ì—ˆë‹¤ê³  ë³´ì‹œë©´ ë©ë‹ˆë‹¤
<br>


## 2. :green_square: í…Œì´ë¸” ìˆ˜ì§‘ ì‹¤ìŠµ

> ì§€í‘œ ìƒì„±ì— í•„ìš”í•œ ê³ ê° ë° ë§¤ì¶œ í…Œì´ë¸”ì„ ì•„íŒŒì¹˜ ìŠ¤ì¿±ì„ í†µí•´ ìˆ˜ì§‘í•©ë‹ˆë‹¤. <br>

ë¡œì»¬ í™˜ê²½ì—ì„œ ëª¨ë“  ë°ì´í„°ë¥¼ ì €ì¥í•´ë‘ì–´ì•¼ í…ŒìŠ¤íŠ¸ ë° ê²€ì¦ì´ í¸í•˜ê¸° ë•Œë¬¸ì— ì €ì¥ì€ ì›ê²© í„°ë¯¸ë„ì˜ ë¡œì»¬ ë””ìŠ¤í¬ì— ì €ì¥ë˜ë©°, ì›ê²© í„°ë¯¸ë„ ì„œë²„ì˜ ë¡œì»¬ ë””ìŠ¤í¬ì™€ ë„ì»¤ì™€ëŠ” ë„ì»¤ ì»´í¬ì¦ˆ íŒŒì¼`docker-compose.yml`ì—ì„œ ë³¼ë¥¨ ë§ˆìš´íŠ¸ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤

### 2-1. *ì›ê²© í„°ë¯¸ë„ì— ì ‘ì†* í›„, *ìŠ¤ì¿± ì»¨í…Œì´ë„ˆì— ì ‘ì†*í•©ë‹ˆë‹¤
```bash
# terminal
cd /home/ubuntu/work/data-engineer-${course}-training/day5
docker-compose up -d
docker-compose ps

echo "ê°ì¢… ì„œë²„ê°€ ê¸°ë™ ë˜ëŠ”ë°ì— ì‹œê°„ì´ ì¢€ ê±¸ë¦½ë‹ˆë‹¤... 10ì´ˆ í›„ì— ì ‘ì†í•©ë‹ˆë‹¤"
sleep 10
docker-compose exec sqoop bash
```

> ì•„ë˜ì™€ ê°™ì€ ë©”ì‹œì§€ê°€ ì¶œë ¥ë˜ê³  ëª¨ë“  ì»¨í…Œì´ë„ˆê°€ ì¢…ë£Œë˜ë©´ ì •ìƒì…ë‹ˆë‹¤

```
[+] Running 4/5
 â ¿ Network default_network  Created   2.9s
 â ¿ Container mysqlStarted   5.6s
 â ¿ Container notebook       Started   9.3s
 â ¿ Container fluentd        Started   7.0s
 â ¿ Container sqoopStarting 10.0s
```
<br>


### 2-2. ì„œë²„ê°€ ì •ìƒ ê¸°ë™ ë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤

```bash
# docker
ask echo hello world
```
> "hello world" ê°€ ì¶œë ¥ë˜ë©´ ì •ìƒì…ë‹ˆë‹¤

<br>


### 2-3. ìˆ˜ì§‘ ëŒ€ìƒ *ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡*ì„ í™•ì¸í•©ë‹ˆë‹¤
```bash
# docker
hostname="mysql"
username="sqoop"
password="sqoop"
```
```bash
# docker
ask sqoop list-databases --connect jdbc:mysql://${hostname}:3306 --username ${username} --password ${password}
```
<details><summary> ì •ë‹µí™•ì¸</summary>

> ì•„ë˜ì˜ ì´ 2ê°œì˜ ë°ì´í„°ë² ì´ìŠ¤ê°€ ì¶œë ¥ë˜ë©´ ì •ë‹µì…ë‹ˆë‹¤ 
`information_schema`
`testdb`

</details>
<br>


### 2-4. ìˆ˜ì§‘ ëŒ€ìƒ *í…Œì´ë¸” ëª©ë¡*ì„ í™•ì¸í•©ë‹ˆë‹¤
```bash
# docker
database="testdb"
```
```bash
# docker
ask sqoop list-tables --connect jdbc:mysql://${hostname}:3306/$database --username ${username} --password ${password}
```
<details><summary> ì •ë‹µí™•ì¸</summary>

> ì•„ë˜ì˜ ì´ 5ê°œì˜ í…Œì´ë¸”ì´ ì¶œë ¥ë˜ë©´ ì •ë‹µì…ë‹ˆë‹¤
`purchase_20201025`
`purchase_20201026`
`seoul_popular_trip`
`user_20201025`
`user_20201026`

</details>
<br>


### 2-5. *ì¼ë³„ ì´ìš©ì í…Œì´ë¸”*ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤
* ê¸°ê°„ : 2020/10/25
* ì €ì¥ : íŒŒì¼€ì´ í¬ë§· <kbd>--as-parquetfile</kbd>
* ê¸°íƒ€ : ê²½ë¡œê°€ ì¡´ì¬í•˜ë©´ ì‚­ì œ í›„ ìˆ˜ì§‘ <kbd>--delete-target-dir</kbd>
* ì†ŒìŠ¤ : <kbd>user\_20201025</kbd>
* íƒ€ê²Ÿ : <kbd>file:///tmp/target/user/20201025</kbd>
```bash
# docker
basename="user"
basedate=""
```

#### 2-5-1. ask ëª…ë ¹ì„ í†µí•´ì„œ ê²°ê³¼ ëª…ë ¹ì–´ë¥¼ í™•ì¸ í›„ì— ì‹¤í–‰í•©ë‹ˆë‹¤
```bash
# docker
ask sqoop import -jt local -m 1 --connect jdbc:mysql://${hostname}:3306/${database} \
--username ${username} --password ${password} --table ${basename}_${basedate} \
--target-dir "file:///tmp/target/${basename}/${basedate}" --as-parquetfile --delete-target-dir
```
<details><summary> ì •ë‹µí™•ì¸</summary>

> `21/07/10 16:27:47 INFO mapreduce.ImportJobBase: Retrieved 5 records.` ì™€ ê°™ì€ ë©”ì‹œì§€ê°€ ì¶œë ¥ë˜ë©´ ì„±ê³µì…ë‹ˆë‹¤

</details>
<br>


### 2-6. *ì¼ë³„ ë§¤ì¶œ í…Œì´ë¸”*ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤
* ê¸°ê°„ : 2020/10/25
* ì €ì¥ : íŒŒì¼€ì´ í¬ë§· <kbd>--as-parquetfile</kbd>
* ê¸°íƒ€ : ê²½ë¡œê°€ ì¡´ì¬í•˜ë©´ ì‚­ì œ í›„ ìˆ˜ì§‘ <kbd>--delete-target-dir</kbd>
* ì†ŒìŠ¤ : <kbd>purchase\_20201025</kbd>
* íƒ€ê²Ÿ : <kbd>file:///tmp/target/purchase/20201025</kbd>
```bash
# docker
basename="purchase"
basedate=""
```

#### 2-6-1. ask ëª…ë ¹ì„ í†µí•´ì„œ ê²°ê³¼ ëª…ë ¹ì–´ë¥¼ í™•ì¸ í›„ì— ì‹¤í–‰í•©ë‹ˆë‹¤
```bash
# docker
ask sqoop import -jt local -m 1 --connect jdbc:mysql://${hostname}:3306/$database \
--username ${username} --password ${password} --table ${basename}_${basedate} \
--target-dir "file:///tmp/target/${basename}/${basedate}" --as-parquetfile --delete-target-dir
```
<details><summary> ì •ë‹µí™•ì¸</summary>

> `21/07/10 16:27:47 INFO mapreduce.ImportJobBase: Retrieved 6 records.` ì™€ ê°™ì€ ë©”ì‹œì§€ê°€ ì¶œë ¥ë˜ë©´ ì„±ê³µì…ë‹ˆë‹¤

</details>
<br>


### 2-7. ëª¨ë“  ë°ì´í„°ê°€ ì •ìƒì ìœ¼ë¡œ ìˆ˜ì§‘ ë˜ì—ˆëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤
> parquet-tools ëŠ” íŒŒì¼€ì´ íŒŒì¼ì˜ ìŠ¤í‚¤ë§ˆ(schema), ì¼ë¶€ë‚´ìš©(head) ë° ì „ì²´ë‚´ìš©(cat)ì„ í™•ì¸í•  ìˆ˜ ìˆëŠ” ì»¤ë§¨ë“œë¼ì¸ ë„êµ¬ì…ë‹ˆë‹¤. ì—°ê´€ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì¡´ì¬í•˜ë¯€ë¡œ hadoop ìŠ¤í¬ë¦½ë¥¼ í†µí•´ì„œ ìˆ˜í–‰í•˜ë©´ í¸ë¦¬í•©ë‹ˆë‹¤

#### 2-7-1. ê³ ê° ë° ë§¤ì¶œ í…Œì´ë¸” ìˆ˜ì§‘ì´ ì˜ ë˜ì—ˆëŠ”ì§€ í™•ì¸ í›„, íŒŒì¼ëª©ë¡ì„ í™•ì¸í•©ë‹ˆë‹¤
```bash
# docker
tree /tmp/target/user
```
```bash
tree /tmp/target/purchase
```
```bash
find /tmp/target -name "*.parquet"
```
#### 2-7-2. ì¶œë ¥ëœ íŒŒì¼ ê²½ë¡œë¥¼ ë³µì‚¬í•˜ì—¬ ê²½ë¡œë¥´ ë³€ìˆ˜ëª…ì— í• ë‹¹í•©ë‹ˆë‹¤
```bash
# docker
filename=""
```

#### 2-7-3. ëŒ€ìƒ íŒŒì¼ê²½ë¡œ ì „ì²´ë¥¼ ë³µì‚¬í•˜ì—¬ ì•„ë˜ì™€ ê°™ì´ ìŠ¤í‚¤ë§ˆë¥¼ í™•ì¸í•©ë‹ˆë‹¤
```bash
# docker
ask hadoop jar /jdbc/parquet-tools-1.8.1.jar schema file://${filename}
```
<details><summary> ì •ë‹µí™•ì¸</summary>

> ì•„ë˜ì™€ ê°™ì€ ì¶œë ¥ì´ ë‚˜ì˜¤ë©´ ì„±ê³µì…ë‹ˆë‹¤
```
message user_20201025 {
  optional int32 u_id;
  optional binary u_name (UTF8);
  optional binary u_gender (UTF8);
  optional int32 u_signup;
}

message purchase_20201025 {
  optional binary p_time (UTF8);
  optional int32 p_uid;
  optional int32 p_id;
  optional binary p_name (UTF8);
  optional int32 p_amount;
}
```

</details>
<br>


#### 2-7-4. íŒŒì¼ ë‚´ìš©ì˜ ë°ì´í„°ê°€ ì •ìƒì ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤
```bash
# docker
ask hadoop jar /jdbc/parquet-tools-1.8.1.jar cat file://${filename}
```
<details><summary> ì •ë‹µí™•ì¸</summary>

> ì•„ë˜ì™€ ê°™ì€ ë°ì´í„°ê°€ ì¶œë ¥ë˜ë©´ ì •ìƒì…ë‹ˆë‹¤

```text
p_time = 1603586155
p_uid = 5
p_id = 2004
p_name = LG TV
p_amount = 2500000
```

</details>
<br>


#### 2-7-5. `ì›ê²© í„°ë¯¸ë„` ì¥ë¹„ì—ë„ ì˜ ì €ì¥ ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤

*  <kbd><samp>Ctrl</samp>+<samp>D</samp></kbd> í˜¹ì€ <kbd>exit</kbd> ëª…ë ¹ìœ¼ë¡œ ì»¨í…Œì´ë„ˆì—ì„œ ë¹ ì ¸ë‚˜ì™€ `ì›ê²© í„°ë¯¸ë„` ë¡œì»¬ ë””ìŠ¤í¬ì— ëª¨ë“  íŒŒì¼ì´ ëª¨ë‘ ìˆ˜ì§‘ë˜ì—ˆë‹¤ë©´ í…Œì´ë¸” ìˆ˜ì§‘ì— ì„±ê³µí•œ ê²ƒì…ë‹ˆë‹¤
```bash
# terminal
cd /home/ubuntu/work/data-engineer-${course}-training/day5
find notebooks -name '*.parquet'
```
<br>


## 3. :green_square: íŒŒì¼ ìˆ˜ì§‘ ì‹¤ìŠµ

### 3-1. *ì›ê²© í„°ë¯¸ë„ì— ì ‘ì†* í›„, *í”Œë£¨ì–¸íŠ¸ë”” ì»¨í…Œì´ë„ˆì— ì ‘ì†*í•©ë‹ˆë‹¤

#### 3-1-1. ì„œë²„ë¥¼ ê¸°ë™í•©ë‹ˆë‹¤ (ì»¨í…Œì´ë„ˆê°€ ì¢…ë£Œëœ ê²½ìš°)

> ì„œë²„ê°€ ì´ë¯¸ ê¸°ë™ë˜ì–´ ìˆëŠ” ê²½ìš° Recreate ë˜ë©°, ì»¨í…Œì´ë„ˆì˜ ìƒíƒœëŠ” ë³„ë„ì˜ ë³¼ë¥¨ì— ì €ì¥ë˜ê³  ìˆìœ¼ë¯€ë¡œ ë¬¸ì œëŠ” ì—†ìŠµë‹ˆë‹¤ë§Œ, ë¼ì´ë¸Œ í™˜ê²½ì—ì„œëŠ” ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤

```bash
# terminal
cd /home/ubuntu/work/data-engineer-${course}-training/day5
docker-compose up -d
docker-compose ps
```

#### 3-1-2. í”Œë£¨ì–¸íŠ¸ë”” ì»¨í…Œì´ë„ˆì— ì ‘ì†í•©ë‹ˆë‹¤
```bash
# docker
docker-compose exec fluentd bash
```

#### 3-1-3. ì´ì „ ì‘ì—…ë‚´ì—­ì„ ëª¨ë‘ ì´ˆê¸°í™” í•˜ê³  ë‹¤ì‹œ ìˆ˜ì§‘í•´ì•¼ í•œë‹¤ë©´ ì•„ë˜ì™€ ê°™ì´ ì •ë¦¬í•©ë‹ˆë‹¤
```bash
# docker
ask rm -rf /tmp/source/access.csv /tmp/source/access.pos /tmp/target/\$\{tag\}/ /tmp/target/access/20201025
```

#### 3-1-4. ë¹„ì–´ìˆëŠ” ì´ìš©ì ì ‘ì†ë¡œê·¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤
```bash
# docker
ask touch /tmp/source/access.csv
```

#### 3-1-5. ìˆ˜ì§‘ ì—ì´ì „íŠ¸ì¸ í”Œë£¨ì–¸íŠ¸ë””ë¥¼ ê¸°ë™ì‹œí‚µë‹ˆë‹¤
```bash
# docker
ask fluentd -c /etc/fluentd/fluent.tail
```
<details> <summary> í”Œë£¨ì–¸íŠ¸ë”” ì„¤ì •ì„ í™•ì¸í•©ë‹ˆë‹¤ </summary>
<p>

```bash
<source>
    @type tail
    @log_level info
    path /tmp/source/access.csv
    pos_file /tmp/source/access.pos
    refresh_interval 5
    multiline_flush_interval 5
    rotate_wait 5
    open_on_every_update true
    emit_unmatched_lines true
    read_from_head false
    tag access
    <parse>
        @type csv
        keys a_time,a_uid,a_id
        time_type unixtime
        time_key a_time
        keep_time_key true
        types a_time:time:unixtime,a_uid:integer,a_id:string
    </parse>
</source>

<match access>
    @type file
    @log_level info
    add_path_suffix true
    path_suffix .json
    path /tmp/target/${tag}/%Y%m%d/access.%Y%m%d.%H%M
    <format>
        @type json
    </format>
    <inject>
        time_key a_timestamp
        time_type string
        timezone +0900
        time_format %Y-%m-%d %H:%M:%S.%L
        tag_key a_tag
    </inject>
    <buffer time,tag>
        timekey 1m
        timekey_use_utc false
        timekey_wait 10s
        timekey_zone +0900
        flush_mode immediate
        flush_thread_count 8
    </buffer>
</match>

<match debug>
    @type stdout
    @log_level debug
</match>
```

</p>
</details>


### 3-2. ë˜ ë‹¤ë¥¸ `ì›ê²© í„°ë¯¸ë„` ì ‘ì† í›„, í”Œë¡œì–¸íŠ¸ë”” ì»¨í…Œì´ë„ˆì— ì ‘ì†í•©ë‹ˆë‹¤
> ê¸°ì¡´ì˜ í„°ë¯¸ë„ì—ì„œëŠ” ë¡œê·¸ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ì—ì´ì „íŠ¸ê°€ ë°ëª¬ìœ¼ë¡œ í•­ìƒ ë™ì‘í•˜ê³  ìˆê¸° ë•Œë¬¸ì—, ë³„ë„ì˜ í„°ë¯¸ë„ì—ì„œ ì‹¤ì œ ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ë™ì‘í•˜ëŠ” ê²ƒì„ ì‹œë®¬ë ˆì´ì…˜ í•˜ê¸° ìœ„í•´ ë³„ë„ì˜ í„°ë¯¸ë„ë¡œ ì ‘ì†í•˜ì—¬ í…ŒìŠ¤íŠ¸ë¥¼ í•©ë‹ˆë‹¤

#### 3-2-1. ìƒˆë¡œìš´ `ì›ê²© í„°ë¯¸ë„`ì„ ì ‘ì†í•©ë‹ˆë‹¤
```bash
# terminal
cd /home/ubuntu/work/data-engineer-${course}-training/day5
docker-compose exec fluentd bash
```

#### 3-2-2. ì‹¤ì œ ë¡œê·¸ê°€ ìŒ“ì´ëŠ” ê²ƒ ì²˜ëŸ¼ access.csv íŒŒì¼ì— ì„ì˜ì˜ ë¡œê·¸ë¥¼ redirect í•˜ì—¬ ë¡œê·¸ë¥¼ append í•©ë‹ˆë‹¤
```bash
# docker
cat /etc/fluentd/access.20201025.csv >> /tmp/source/access.csv
```

#### 3-2-3. ìˆ˜ì§‘ëœ ë¡œê·¸ê°€ ì •ìƒì ì¸ JSON íŒŒì¼ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤
```bash
# docker
tree -L 2 /tmp/target
```
```bash
find /tmp/target -name '*.json' | head
```

#### 3-2-4. ì›ë³¸ ë¡œê·¸ì™€, ìµœì¢… ìˆ˜ì§‘ëœ ë¡œê·¸ì˜ ë ˆì½”ë“œ ìˆ˜ê°€ ê°™ì€ì§€ í™•ì¸í•©ë‹ˆë‹¤
```bash
# docker
cat `find /tmp/target/access/20201025 -name '*.json'` | wc -l
```
```bash
wc -l /tmp/source/access.csv
```

<details><summary> ì •ë‹µí™•ì¸</summary>

> ìˆ˜ì§‘ëœ íŒŒì¼ì˜ ë¼ì¸ ìˆ˜ì™€, ì›ë³¸ ë¡œê·¸ì˜ ë¼ì¸ ìˆ˜ê°€ 12 ë¼ì¸ìœ¼ë¡œ ì¼ì¹˜í•œë‹¤ë©´ ì •ìƒì…ë‹ˆë‹¤ 

</details>


#### 3-2-5. `ì›ê²© í„°ë¯¸ë„ ` ë¡œì»¬ ìŠ¤í† ë¦¬ì§€ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤

* ê¸°ë™ë˜ì–´ ìˆëŠ” fluentd ì• í”Œë¦¬ì¼€ì´ì…˜ì€ <kbd><samp>Ctrl</samp>+<samp>C</samp></kbd> ëª…ë ¹ìœ¼ë¡œ ì¢…ë£Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
* ê¸°ë™ë˜ì–´ ìˆëŠ” docker ì»¨í…Œì´ë„ˆëŠ” í„°ë¯¸ë„ í™”ë©´ì—ì„œ <kbd><samp>Ctrl</samp>+<samp>D</samp></kbd> í˜¹ì€ <kbd>exit</kbd> ëª…ë ¹ìœ¼ë¡œ ì»¨í…Œì´ë„ˆì—ì„œ ë¹ ì ¸ë‚˜ì™€ `ì›ê²© í„°ë¯¸ë„` ë¡œì»¬ ë””ìŠ¤í¬ì— JSON íŒŒì¼ì´ í™•ì¸ ë˜ì—ˆë‹¤ë©´ ì›¹ ë¡œê·¸ ìˆ˜ì§‘ì— ì„±ê³µí•œ ê²ƒì…ë‹ˆë‹¤
* ì—´ë ¤ ìˆëŠ” 2ê°œ í„°ë¯¸ë„ ëª¨ë‘ ì¢…ë£Œí•©ë‹ˆë‹¤
```bash
# terminal
find notebooks -name '*.json'
```
<br>


## 4. :green_square: ë…¸íŠ¸ë¶ ì»¨í…Œì´ë„ˆ ê¸°ë™

> ë³¸ ì¥ì—ì„œ ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ë°ì´í„° ë³€í™˜ ë° ì§€í‘œ ìƒì„±ì‘ì—…ì„ ìœ„í•˜ì—¬ ì£¼í”¼í„° ë…¸íŠ¸ë¶ì„ ì—´ì–´ë‘¡ë‹ˆë‹¤

### 4-1. ë…¸íŠ¸ë¶ ì£¼ì†Œë¥¼ í™•ì¸í•˜ê³ , í¬ë¡¬ ë¸Œë¼ìš°ì €ë¡œ ì ‘ì†í•©ë‹ˆë‹¤

#### 4-1-1. ë…¸íŠ¸ë¶ ê¸°ë™ ë° í™•ì¸
```bash
# terminal
docker-compose logs notebook | grep 8888
```
> ì¶œë ¥ëœ  URLì„ ë³µì‚¬í•˜ì—¬ `127.0.0.1:8888` ëŒ€ì‹  ê°œì¸ `<hostname>.aiffelbiz.co.kr:8888` ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ í¬ë¡¬ ë¸Œë¼ìš°ì €ë¥¼ í†µí•´ ì ‘ì†í•˜ë©´, jupyter notebook lab ì´ ì—´ë¦¬ê³  work í´ë”ê°€ ë³´ì´ë©´ ì •ìƒê¸°ë™ ëœ ê²ƒì…ë‹ˆë‹¤

#### 4-1-2. ê¸° ìƒì„±ëœ ì‹¤ìŠµìš© ë…¸íŠ¸ë¶ì„ ì—½ë‹ˆë‹¤
* ì¢Œì¸¡ ë©”ë‰´ì—ì„œ "data-engineer-lgde-day5.ipynb" ì„ ë”ë¸”í´ë¦­í•©ë‹ˆë‹¤

#### 4-1-3. ì‹ ê·œë¡œ ë…¸íŠ¸ë¶ì„ ë§Œë“¤ê³  ì‹¶ì€ ê²½ìš°
* `Launcher` íƒ­ì—ì„œ `Notebook - Python 3` ë¥¼ ì„ íƒí•˜ê³ 
* íƒ­ì—ì„œ `Rename Notebook...` ë©”ë‰´ë¥¼ í†µí•´ ì´ë¦„ì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤

<br>


## 5. :green_square: ìˆ˜ì§‘ëœ ë°ì´í„° íƒìƒ‰

> ìŠ¤íŒŒí¬ ì„¸ì…˜ì„ í†µí•´ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°ì˜ í˜•íƒœë¥¼ íŒŒì•…í•˜ê³ , ìŠ¤íŒŒí¬ì˜ ê¸°ë³¸ ëª…ë ¹ì–´ë¥¼ í†µí•´ ìˆ˜ì§‘ëœ ë°ì´í„° ì§‘í•©ì„ íƒìƒ‰í•©ë‹ˆë‹¤

### 5-1. ìŠ¤íŒŒí¬ ì„¸ì…˜ ìƒì„±

#### 5-1-1. ìŠ¤íŒŒí¬ ê°ì²´ë¥¼ ìƒì„±í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ê³ , <kbd><kbd>Shift</kbd>+<kbd>Enter</kbd></kbd> ë¡œ ìŠ¤íŒŒí¬ ë²„ì „ì„ í™•ì¸í•©ë‹ˆë‹¤
```python
# ì½”ì–´ ìŠ¤íŒŒí¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸ í•©ë‹ˆë‹¤
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

spark = (
    SparkSession
    .builder
    .appName("Data Engineer Training Course")
    .config("spark.sql.session.timeZone", "Asia/Seoul")
    .getOrCreate()
)

# ë…¸íŠ¸ë¶ì—ì„œ í…Œì´ë¸” í˜•íƒœë¡œ ë°ì´í„° í”„ë ˆì„ ì¶œë ¥ì„ ìœ„í•œ ì„¤ì •ì„ í•©ë‹ˆë‹¤
from IPython.display import display, display_pretty, clear_output, JSON
spark.conf.set("spark.sql.repl.eagerEval.enabled", True) # display enabled
spark.conf.set("spark.sql.repl.eagerEval.truncate", 100) # display output columns size
spark
```
<details><summary> ì •ë‹µí™•ì¸</summary>

> ìŠ¤íŒŒí¬ ì—”ì§„ì˜ ë²„ì „ `v3.0.1`ì´ ì¶œë ¥ë˜ë©´ ì„±ê³µì…ë‹ˆë‹¤

</details>
<br> 


#### 5-1-2. ìˆ˜ì§‘ëœ í…Œì´ë¸”ì„ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì½ê³ , ìŠ¤í‚¤ë§ˆ ë° ë°ì´í„° ì¶œë ¥í•˜ê¸°

> íŒŒì¼€ì´ í¬ë§·ì˜ ê²½ìš°ëŠ” ëª…ì‹œì ì¸ ìŠ¤í‚¤ë§ˆ ì •ì˜ê°€ ë˜ì–´ ìˆì§€ë§Œ, Json í¬ë§·ì˜ ê²½ìš°ëŠ” ë°ì´í„°ì˜ ê°’ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ë°ì´í„°ë¥¼ ì½ì–´ë“¤ì¼ ë•Œì— ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤

* ë°ì´í„°í”„ë ˆì„ ë³€ìˆ˜ëª… ë° ê²½ë¡œëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤
  - 2020/10/25 ì¼ì ê³ ê° (parquet) : <kbd>user25</kbd> <- <kbd>user/20201025</kbd> 
  - 2020/10/25 ì¼ì ë§¤ì¶œ (parquet) : <kbd>purchase25</kbd> <- <kbd>purchase/20201025</kbd> 
  - 2020/10/25 ì¼ì ì ‘ì† (json) : <kbd>access25</kbd> <- <kbd>access/20201025</kbd> 

* ì•„ë˜ì˜ ì œì•½ì¡°ê±´ì„ ë§Œì¡± ì‹œì¼œì•¼ í•©ë‹ˆë‹¤
  - ì…ë ¥ í¬ë§·ì´ Json ì¸ ê²½ìš°ëŠ” json ëª…ë ¹ì–´ì™€ ì¶”ì •(infer) ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš” <kbd>spark.read.option("inferSchema", "true").json("access/20201024")</kbd> 
  - ëª¨ë“  ë°ì´í„°ì— ëŒ€í•œ ìŠ¤í‚¤ë§ˆë¥¼ ì¶œë ¥í•˜ì„¸ìš” <kbd>dataFrame.printSchema()</kbd> 
  - ë°ì´í„° ë‚´ìš©ì„ ì¶œë ¥í•˜ì—¬ í™•ì¸í•˜ì„¸ìš” <kbd>dataFrame.show() í˜¹ì€ display(dataFrame)</kbd> 

* ê³ ê° ì •ë³´ íŒŒì¼ì„ ì½ê³ , ìŠ¤í‚¤ë§ˆì™€ ë°ì´í„° ì¶œë ¥í•˜ê¸°
```python
user25 = spark.read.parquet("user/20201025")
user25.printSchema()
user25.show(truncate=False)
display(user25)
```

* ë§¤ì¶œ ì •ë³´ íŒŒì¼ì„ ì½ê³ , ìŠ¤í‚¤ë§ˆì™€ ë°ì´í„° ì¶œë ¥í•˜ê¸°
```python
purchase25 = <ë§¤ì¶œ ë°ì´í„° ê²½ë¡œì—ì„œ ì½ì–´ì„œ ìŠ¤í‚¤ë§ˆì™€, ë°ì´í„°ë¥¼ ì¶œë ¥í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”>
```

* ì ‘ì† ì •ë³´ íŒŒì¼(json)ì„ ì½ê³ , ìŠ¤í‚¤ë§ˆì™€ ë°ì´í„° ì¶œë ¥í•˜ê¸°
```python
access25 = <ì ‘ì† ë°ì´í„° ê²½ë¡œì—ì„œ ì½ì–´ì„œ ìŠ¤í‚¤ë§ˆì™€, ë°ì´í„°ë¥¼ ì¶œë ¥í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”>
```
<details><summary> ì •ë‹µí™•ì¸</summary>

> ê³ ê°, ë§¤ì¶œ ë° ì ‘ì† ë°ì´í„°ì˜ ìŠ¤í‚¤ë§ˆì™€, ë°ì´í„°ê°€ ëª¨ë‘ ì¶œë ¥ë˜ë©´ ì„±ê³µì…ë‹ˆë‹¤

</details>
<br>


### 5-2. ìˆ˜ì§‘ëœ ê³ ê°, ë§¤ì¶œ ë° ì ‘ì† ì„ì‹œ í…Œì´ë¸” ìƒì„±

#### 5-2-1. ë°ì´í„°í”„ë ˆì„ì„ ì´ìš©í•˜ì—¬ ì„ì‹œí…Œì´ë¸” ìƒì„±í•˜ê¸°
```python
user25.createOrReplaceTempView("user25")
purchase25.createOrReplaceTempView("purchase25")
access25.createOrReplaceTempView("access25")
spark.sql("show tables '*25'")
```
<details><summary> ì •ë‹µí™•ì¸</summary>

> `show tables` ê²°ê³¼ë¡œ `user25`, `purchase25`, `access25` 3ê°œ í…Œì´ë¸”ì´ ì¶œë ¥ë˜ë©´ ì„±ê³µì…ë‹ˆë‹¤

</details>
<br>


### 5-3. SparkSQLì„ ì´ìš©í•˜ì—¬ í…Œì´ë¸” ë³„ ë°ì´í„°í”„ë ˆì„ ìƒì„±í•˜ê¸°

#### 5-3-1. ì•„ë˜ì— ë¹„ì–´ìˆëŠ” ì¡°ê±´ì„ ì±„ì›Œì„œ ì˜¬ë°”ë¥¸ ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”

* ì•„ë˜ì˜ ì¡°ê±´ì´ ë§Œì¡±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤
  - 2020/10/25 ì— ë“±ë¡(`u_signup`)ëœ ìœ ì €ë§Œ í¬í•¨ë  ê²ƒ <kbd>`u_signup` >= '20201025' and `u_signup` < '20201026'</kbd>
  - 2020/10/25 ì— ë°œìƒí•œ ë§¤ì¶œ(`p_time`)ë§Œ í¬í•¨í•  ê²ƒ <kbd>`p_time` >= '2020-10-25 00:00:00' and `p_time` < '2020-10-26 00:00:00'</kbd>

```python
u_signup_condition = "<10ì›” 25ì¼ìì— ë“±ë¡ëœ ìœ ì €ë§Œ í¬í•¨ë˜ëŠ” ì¡°ê±´ì„ ì‘ì„±í•©ë‹ˆë‹¤>"
user = spark.sql("select u_id, u_name, u_gender from user25").where(u_signup_condition)
user.createOrReplaceTempView("user")

p_time_condition = "<10ì›” 25ì¼ìì— ë°œìƒí•œ ë§¤ì¶œë§Œ í¬í•¨ë˜ëŠ” ì¡°ê±´ì„ ì‘ì„±í•©ë‹ˆë‹¤>"
purchase = spark.sql("select from_unixtime(p_time) as p_time, p_uid, p_id, p_name, p_amount from purchase25").where(p_time_condition)
purchase.createOrReplaceTempView("purchase")

access = spark.sql("select a_id, a_tag, a_timestamp, a_uid from access25")
access.createOrReplaceTempView("access")

spark.sql("show tables")
```
<details><summary> ì •ë‹µí™•ì¸</summary>

> `show tables` ê²°ê³¼ì— ì´ 6ê°œì˜ í…Œì´ë¸”ì´ ì¶œë ¥ë˜ë©´ ì„±ê³µì…ë‹ˆë‹¤

</details>
<br>


### 5-4. ìƒì„±ëœ í…Œì´ë¸”ì„ SQL ë¬¸ì„ ì´ìš©í•˜ì—¬ íƒìƒ‰í•˜ê¸°
> SQL í˜¹ì€ DataFrame API ì–´ëŠ ìª½ì„ ì´ìš©í•˜ì—¬ë„ ë¬´ê´€í•˜ë©°, ê²°ê³¼ë§Œ ë™ì¼í•˜ê²Œ ë‚˜ì˜¤ë©´ ì •ë‹µì…ë‹ˆë‹¤

#### 5-4-1. í•œ ìª½ì˜ ì„±ë³„('ë‚¨' í˜¹ì€ 'ì—¬')ì„ ê°€ì§„ ëª©ë¡ì„ ì¶œë ¥í•˜ì„¸ìš”
```python
spark.sql("describe user")
# whereCondition = "<ì„±ë³„ì„ êµ¬ë³„í•˜ëŠ” ì¡°ê±´ì„ ì‘ì„±í•˜ì„¸ìš”>"
# spark.sql("select * from user").where(whereCondition)
```

#### 5-4-2. ìƒí’ˆê¸ˆì•¡ì´ 200ë§Œì›ì„ ì´ˆê³¼í•˜ëŠ” ë§¤ì¶œ ëª©ë¡ì„ ì¶œë ¥í•˜ì„¸ìš”
```python
spark.sql("describe purchase")
# selectClause = "<ê¸ˆì•¡ì„ í•„í„°í•˜ëŠ” ì¡°ê±´ì„ ì‘ì„±í•˜ì„¸ìš”>"
# spark.sql(selectClause)
```

#### 5-4-3. GroupBy êµ¬ë¬¸ì„ ì´ìš©í•˜ì—¬ ë¡œê·¸ì¸, ë¡œê·¸ì•„ì›ƒ íšŸìˆ˜ë¥¼ ì¶œë ¥í•˜ì„¸ìš”
```python
spark.sql("describe access")
# groupByClause="<ë¡œê·¸ì¸/ì•„ì›ƒ ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„í•˜ëŠ” êµ¬ë¬¸ì„ ì‘ì„±í•˜ì„¸ìš”>"
# spark.sql(groupByClause)
```
<details><summary> ì •ë‹µí™•ì¸</summary>

> ìœ„ì—ì„œë¶€í„° ê°ê° "ë‚¨:3, ì—¬:2", "2ê°œ", "login:7, logout:5" ì´ ë‚˜ì˜¤ë©´ ì •ë‹µì…ë‹ˆë‹¤

</details>
<br>


## 6. :green_square: ê¸°ë³¸ ì§€í‘œ ìƒì„±

> ìƒì„±ëœ í…Œì´ë¸”ì„ í†µí•˜ì—¬ ê¸°ë³¸ ì§€í‘œ(DAU, DPU, DR, ARPU, ARPPU) ë¥¼ ìƒì„±í•©ë‹ˆë‹¤

### 6-1. DAU (Daily Activer User) ì§€í‘œë¥¼ ìƒì„±í•˜ì„¸ìš”

* ì•„ë˜ì˜ ì¡°ê±´ì´ ë§Œì¡±í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”
  - ì§€í‘œì •ì˜ : ì§€ì •í•œ ì¼ìì˜ ì ‘ì†í•œ ìœ ì € ìˆ˜ <kbd>count(distinct `a_uid`)</kbd>
  - ì§€í‘œì‚°ì‹ : ì§€ì •í•œ ì¼ìì˜ ì ‘ì† í…Œì´ë¸”ì— ë¡œê·¸(ë¡œê·¸ì¸ í˜¹ì€ ë¡œê·¸ì•„ì›ƒ)ê°€ í•œ ë²ˆ ì´ìƒ ë°œìƒí•œ ì´ìš©ìì˜ ë¹ˆë„ìˆ˜
  - ì…ë ¥í˜•íƒœ : access í…Œì´ë¸”
  - ì¶œë ¥í˜•íƒœ : number (ì»¬ëŸ¼ëª…: DAU)

```python
display(access)
# distinctAccessUser = "select <ê³ ê°ìˆ˜ ì§‘ê³„í•¨ìˆ˜> as DAU from access"
# dau = spark.sql(distinctAccessUser)
# display(dau)
```
<details><summary> ì •ë‹µí™•ì¸</summary>

> "DAU : 5"ê°€ ë‚˜ì˜¤ë©´ ì •ë‹µì…ë‹ˆë‹¤ 

</details>
<br>


### 6-2. DPU (Daily Paying User) ì§€í‘œë¥¼ ìƒì„±í•˜ì„¸ìš”

* ì•„ë˜ì˜ ì¡°ê±´ì´ ë§Œì¡±í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”
  - ì§€í‘œì •ì˜ : ì§€ì •í•œ ì¼ìì˜ êµ¬ë§¤ ìœ ì € ìˆ˜ <kbd>count(distinct `p_uid`)</kbd>
  - ì§€í‘œì‚°ì‹ : ì§€ì •í•œ ì¼ìì˜ êµ¬ë§¤ í…Œì´ë¸”ì— í•œ ë²ˆì´ë¼ë„ êµ¬ë§¤ê°€ ë°œìƒí•œ ì´ìš©ìì˜ ë¹ˆë„ìˆ˜
  - ì…ë ¥í˜•íƒœ : purchase í…Œì´ë¸”
  - ì¶œë ¥í˜•íƒœ : number (ì»¬ëŸ¼ëª…: PU)

```python
display(purchase)
# distinctPayingUser = "<êµ¬ë§¤ ê³ ê°ìˆ˜ ì§‘ê³„í•¨ìˆ˜>"
# pu = spark.sql(distinctPayingUser)
# display(pu)
```
<details><summary> ì •ë‹µí™•ì¸</summary>

> "PU : 4"ê°€ ë‚˜ì˜¤ë©´ ì •ë‹µì…ë‹ˆë‹¤

</details>
<br>


### 6-3. DR (Daily Revenue) ì§€í‘œë¥¼ ìƒì„±í•˜ì„¸ìš”

* ì•„ë˜ì˜ ì¡°ê±´ì´ ë§Œì¡±í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”
  - ì§€í‘œì •ì˜ : ì§€ì •í•œ ì¼ìì— ë°œìƒí•œ ì´ ë§¤ì¶œ ê¸ˆì•¡ <kbd>sum(`p_amount`)</kbd>
  - ì§€í‘œì‚°ì‹ : ì§€ì •í•œ ì¼ìì˜ êµ¬ë§¤ í…Œì´ë¸”ì— ì €ì¥ëœ ì „ì²´ ë§¤ì¶œ ê¸ˆì•¡ì˜ í•©
  - ì…ë ¥í˜•íƒœ : access í…Œì´ë¸”
  - ì¶œë ¥í˜•íƒœ : number (ì»¬ëŸ¼ëª…: DR)

```python
display(purchase)
# sumOfDailyRevenue = "<ì¼ ë³„ êµ¬ë§¤ê¸ˆì•¡ ì§‘ê³„í•¨ìˆ˜>"
# dr = spark.sql(sumOfDailyRevenue)
# display(dr)
```
<details><summary> ì •ë‹µí™•ì¸</summary>

> "DR : 12200000" ì´ ë‚˜ì˜¤ë©´ ì •ë‹µì…ë‹ˆë‹¤

</details>
<br> 


### 6-4. ARPU (Average Revenue Per User) ì§€í‘œë¥¼ ìƒì„±í•˜ì„¸ìš”

* ì•„ë˜ì˜ ì¡°ê±´ì´ ë§Œì¡±í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”
  - ì§€í‘œì •ì˜ : ìœ ì € ë‹¹ í‰ê·  ë°œìƒ ë§¤ì¶œ ê¸ˆì•¡
  - ì§€í‘œì‚°ì‹ : ì´ ë§¤ì¶œ / ì „ì²´ ìœ ì € ìˆ˜ = DR / DAU
  - ì…ë ¥í˜•íƒœ : Daily Revenue, Daily Active User
  - ì¶œë ¥í˜•íƒœ : number (ë¬¸ìì—´: ARPU )

```python
v_dau = dau.collect()[0]["DAU"]
v_pu = pu.collect()[0]["PU"]
v_dr = dr.collect()[0]["DR"]

# print("ARPU : {}".format(<ìœ ì €ë‹¹ ë§¤ì¶œ ê¸ˆì•¡ ê³„ì‚°ì‹>))
```
<details><summary> ì •ë‹µí™•ì¸</summary>

> "ARPU : 2440000.0" ê°€ ë‚˜ì˜¤ë©´ ì •ë‹µì…ë‹ˆë‹¤

</details>
<br>


### 6-5. ARPPU (Average Revenue Per Paying User) ì§€í‘œë¥¼ ìƒì„±í•˜ì„¸ìš”

* ì•„ë˜ì˜ ì¡°ê±´ì´ ë§Œì¡±í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”
  - ì§€í‘œì •ì˜ : ìœ ì € ë‹¹ í‰ê·  ë°œìƒ ë§¤ì¶œ ê¸ˆì•¡
  - ì§€í‘œì‚°ì‹ : ì´ ë§¤ì¶œ / ì „ì²´ ìœ ì € ìˆ˜ = DR / PU
  - ì…ë ¥í˜•íƒœ : Daily Revenue, Daily Paying User
  - ì¶œë ¥í˜•íƒœ : number

```python
# print("ARPPU : {}".format(<êµ¬ë§¤ìœ ì € ë‹¹ ë§¤ì¶œ ê¸ˆì•¡ ê³„ì‚°ì‹>))
```
<details><summary> ì •ë‹µí™•ì¸</summary>

> "ARPPU : 3050000.0" ê°€ ë‚˜ì˜¤ë©´ ì •ë‹µì…ë‹ˆë‹¤

</details>
<br>


## 7. :blue_square: ê³ ê¸‰ ì§€í‘œ ìƒì„±

### 7-1. ë””ë©˜ì ¼ í…Œì´ë¸”ì„ ì„¤ê³„ í•©ë‹ˆë‹¤

* ì•„ë˜ì˜ ì¡°ê±´ì´ ë§Œì¡±í•´ì•¼ í•©ë‹ˆë‹¤
  - ì§€í‘œì •ì˜ : ì´ìš©ì ëˆ„ì  ìƒíƒœ ì •ë³´
  - ì§€í‘œì‚°ì‹ : ì˜¤ëŠ˜ê¹Œì§€ ì ‘ì†í•œ ëª¨ë“  ìœ ì €ì˜ ì •ë³´ë¥¼ ì €ì¥í•˜ëŠ” í…Œì´ë¸”
  - ì…ë ¥í˜•íƒœ : user, purchase, access
  - ì¶œë ¥í˜•íƒœ : ì•„ë˜ì™€ ê°™ì´ ì„¤ê³„í•©ë‹ˆë‹¤

* ë””ë©˜ì ¼ í…Œì´ë¸”ì˜ ìŠ¤í‚¤ë§ˆëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤

| ì»¬ëŸ¼ëª… | ì»¬ëŸ¼íƒ€ì… | ì„¤ëª… |
| --- | --- | --- |
| `d_uid` | integer | ì•„ì´ë”” |
| `d_name` | string | ì´ë¦„ |
| `d_pamount` | integer | ëˆ„ì  êµ¬ë§¤ ê¸ˆì•¡ |
| `d_pcount` | integer | ëˆ„ì  êµ¬ë§¤ íšŸìˆ˜ |
| `d_acount` | integer | ëˆ„ì  ì ‘ì† íšŸìˆ˜ |
| `d_first_purchase` | string | ìµœì´ˆ êµ¬ë§¤ ì¼ì‹œ |
| `dt` | string | ìœ ì €ì•„ì´ë”” |

> ë””ë©˜ì ¼ í…Œì´ë¸”ì„ í†µí•´ì„œ ì–´ë–»ê²Œ ìƒì„±í•  ê²ƒì¸ì§€ ìƒê°í•´ ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤
<br>


### 7-2. ì˜¤í”ˆ ì²« ë‚  ì ‘ì†í•œ ëª¨ë“  ê³ ê° ë° ì ‘ì† íšŸìˆ˜ë¥¼ ê°€ì§„ ë°ì´í„°í”„ë ˆì„ì„ ìƒì„±í•©ë‹ˆë‹¤

* ì•„ë˜ì˜ ì¡°ê±´ì´ ë§Œì¡±í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”
  - ì§€í‘œì •ì˜ : ì˜¤ëŠ˜ ì ‘ì†í•œ ì´ìš©ì <kbd>select `a_uid`, count(`a_uid`) ... group by `a_uid`</kbd>
  - ì§€í‘œì‚°ì‹ : ì ‘ì† ì—¬ë¶€ëŠ” 'login' ë¡œê·¸ê°€ ì¡´ì¬í•˜ë©´ ì ‘ì†í•œ ìœ ì €ë¡œ ê°€ì •
  - ì…ë ¥í˜•íƒœ : user
  - ì¶œë ¥í˜•íƒœ : `a_uid`, `a_count`
  - ì •ë ¬í˜•íƒœ : `a_uid` asc

* access í…Œì´ë¸”ë¡œë¶€í„° `a_uid` ê°€ 'login' ì¸ `a_uid` ê°’ì˜ ë¹ˆë„ìˆ˜ë¥¼ group by `a_uid` ì§‘ê³„ë¥¼ í†µí•´ êµ¬í•˜ì‹œì˜¤
```python
access.printSchema()
# countOfAccess = "select a_uid, <ì§‘ê³„í•¨ìˆ˜> from user <ì§‘ê³„ êµ¬ë¬¸>"
# accs = spark.sql(countOfAccess)
# display(accs)
```
<details><summary> ì •ë‹µí™•ì¸</summary>

> ì ‘ì† ë¹ˆë„ê°€ ê°€ì¥ ë†’ì€ ì´ìš©ìëŠ” `a_id` ê°€ "2, 4"ë²ˆìœ¼ë¡œ 2ëª…ì´ ë‚˜ì˜¤ë©´ ì •ë‹µì…ë‹ˆë‹¤

</details>
<br>


### 7-3. ì¼ ë³„ ì´ìš©ì ë³„ ì´ ë§¤ì¶œ ê¸ˆì•¡ê³¼, êµ¬ë§¤ íšŸìˆ˜ë¥¼ ê°€ì§€ëŠ” ë°ì´í„°í”„ë ˆì„ì„ ìƒì„±í•©ë‹ˆë‹¤

* ì•„ë˜ì˜ ì¡°ê±´ì´ ë§Œì¡±í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”
  - ì§€í‘œì •ì˜ : ì´ìš©ì ë³„ ë§¤ì¶œ íšŸìˆ˜ì™€, í•© <kbd>select `p_uid`, count(`p_uid`), sum(`p_amount`) ... group by `p_uid`</kbd>
  - ì§€í‘œì‚°ì‹ : ë§¤ì¶œ ê¸ˆì•¡ì„ ì´ìš©ì ë³„ ì§‘ê³„ë¥¼ í•©ë‹ˆë‹¤
  - ì…ë ¥í˜•íƒœ : purchase
  - ì¶œë ¥í˜•íƒœ : `p_uid`, `p_count`, `p_amount`
  - ì •ë ¬í˜•íƒœ : `p_count` desc`, p_amount` desc

* purchase í…Œì´ë¸”ë¡œ ë¶€í„° `p_uid` ë³„ ë§¤ì¶œ íšŸìˆ˜(count)ì™€, ë§¤ì¶œ ê¸ˆì•¡ì˜ í•©(sum)ì„ êµ¬í•˜ëŠ” ì§‘ê³„ ì¿¼ë¦¬ë¥¼ ìƒì„± í•˜ì‹œì˜¤
```python
purchase.printSchema()
# sumOfCountAndAmount = "select p_uid, <ë¹ˆë„ ì§‘ê³„í•¨ìˆ˜>, <ë§¤ì¶œ ì§‘ê³„í•¨ìˆ˜> from purchase <ì§‘ê³„ì¡°ê±´>"
# amts = spark.sql(sumOfCountAndAmount)
# display(amts)
```
<details><summary> ì •ë‹µí™•ì¸</summary>

> ë§¤ì¶œ ê¸ˆì•¡ì´ 600ë§Œì›ì— 2íšŒ ë°œìƒí•œ 5ë²ˆ ìœ ì €ê°€ 1ìœ„ì´ë©´ ì •ë‹µì…ë‹ˆë‹¤

</details>
<br>


### 7-4. ì´ìš©ì ì •ë³´ì™€ êµ¬ë§¤ ì •ë³´ì™€ ì¡°ì¸í•©ë‹ˆë‹¤

* ì•„ë˜ì˜ ì¡°ê±´ì´ ë§Œì¡±í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”
  - ì§€í‘œì •ì˜ : ì´ìš©ì ë°ì´í„°ì™€ ë§¤ì¶œ ì •ë³´ì˜ ê²°í•© <kbd>leftSide.join(rightSide, joinCondition, joinHow)</kbd>
  - ì§€í‘œì‚°ì‹ : ì´ìš©ìëŠ” ë°˜ë“œì‹œ ì¡´ì¬í•´ì•¼í•˜ë©°, ë§¤ì¶œì€ ì—†ì–´ë„ ë©ë‹ˆë‹¤ (`left_outer`)
  - ì…ë ¥í˜•íƒœ : accs, amts
  - ì¶œë ¥í˜•íƒœ : `a_uid`, `a_count`, `p_uid`, `p_count`, `p_amount`
  - ì •ë ¬í˜•íƒœ : `a_uid` asc

* 7-1 ì—ì„œ ìƒì„±í•œ accs ì™€ 7-2 ì—ì„œ ìƒì„±í•œ amts ë°ì´í„°ë¥¼ uid ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ left outer ì¡°ì¸ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤
```python
accs.printSchema()
amts.printSchema()
# joinCondition = <ê³ ê°ê³¼ ë§¤ì¶œ ì¡°ì¸ ì¡°ê±´>
# joinHow = "<ì¡°ì¸ ë°©ì‹>"
# dim1 = accs.join(amts, joinCondition, joinHow)
# dim1.printSchema()
# display(dim1.orderBy(asc("a_uid")))
```
<details><summary> ì •ë‹µí™•ì¸</summary>

> uid ê°€ 4ë²ˆì¸ ì´ìš©ìë§Œ ë§¤ì¶œ ì •ë³´ê°€ null ì´ë©´ ì •ë‹µì…ë‹ˆë‹¤

</details>
<br>


### 7-5. ê³ ê° ì •ë³´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤

* ì•„ë˜ì˜ ì¡°ê±´ì´ ë§Œì¡±í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”
  - ì§€í‘œì •ì˜ : ê³ ê°ì˜ ì´ë¦„ê³¼ ì„±ë³„ì„ ì¶”ê°€í•©ë‹ˆë‹¤ 
  - ì§€í‘œì‚°ì‹ : dim1 í…Œì´ë¸”ê³¼ ê³ ê°ì •ë³´ë¥¼ ê²°í•©í•©ë‹ˆë‹¤
  - ì…ë ¥í˜•íƒœ : dim1, user
  - ì¶œë ¥í˜•íƒœ : `a_uid`, `a_count`, `p_uid`, `p_count`, `p_amount`, `u_id`, `u_name`, `u_gender`
  - ì •ë ¬í˜•íƒœ : `a_uid` asc

* 7-4 ì—ì„œ ìƒì„±í•œ dim1 ë°ì´í„°ì™€ user ë°ì´í„°ë¥¼ ê²°í•©í•©ë‹ˆë‹¤
```python
dim1.printSchema()
user.printSchema()
# joinCondition = <ë””ë©˜ì ¼ê³¼ ê³ ê°ì •ë³´ ì¡°ì¸ ì¡°ê±´>
# joinHow = "<ì¡°ì¸ ë°©ì‹>"
# dim2 = dim1.join(user, joinCondition, joinHow)
# dim2.printSchema()
# display(dim2.orderBy(asc("a_uid")))
```
<details><summary> ì •ë‹µí™•ì¸</summary>

> uid ê°€ 4ë²ˆì¸ ì´ìš©ìë§Œ ë§¤ì¶œ ì •ë³´ê°€ null ì´ë©´ ì •ë‹µì…ë‹ˆë‹¤

</details>
<br>


### 7-6. ì¤‘ë³µë˜ëŠ” ID ì»¬ëŸ¼ì€ ì œê±°í•˜ê³ , ìˆ«ì í•„ë“œì— ë„ê°’ì€ 0ìœ¼ë¡œ ê¸°ë³¸ê°’ì„ ë„£ì–´ì¤ë‹ˆë‹¤

* ì•„ë˜ì˜ ì¡°ê±´ì´ ë§Œì¡±í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”
  - ì§€í‘œì •ì˜ : ì¤‘ë³µë˜ëŠ” uid ì»¬ëŸ¼ì„ ì œê±°í•˜ê³ , ë§¤ì¶œì •ë³´ê°€ ì—†ëŠ” ê³„ì •ì€ 0ìœ¼ë¡œ ê¸°ë³¸ê°’ì„ ë„£ì–´ì¤ë‹ˆë‹¤
  - ì§€í‘œì‚°ì‹ : <kbd>fillDefaultValue = {"`p_amount`":0, "`p_count`":0}</kbd>
  - ì…ë ¥í˜•íƒœ : dim2
  - ì¶œë ¥í˜•íƒœ : `a_uid`, `a_count`, `p_amount`, `p_count`, `u_name`, `u_gender`
  - ì •ë ¬í˜•íƒœ : `a_uid` asc

* 7-4 ì—ì„œ ìƒì„±í•œ dim1 ë°ì´í„°ì™€ user ë°ì´í„°ë¥¼ ê²°í•©í•˜ë˜, ì¤‘ë³µ ì»¬ëŸ¼ì€ ì œê±°í•˜ê³ , ê¸°ë³¸ê°’ì„ 0ìœ¼ë¡œ ì±„ìš°ëŠ” `fillDefaultValue`ë¥¼ ì‘ì„±í•˜ì„¸ìš”
```python
dim2.printSchema()
dim3 = dim2.drop("p_uid", "u_id")
# fillDefaultValue = {<ê¸°ë³¸ê°’ì„ ë„£ì„ ì»¬ëŸ¼ê³¼ ê¸°ë³¸ê°’ ì‚¬ì „> }
# dim4 = dim3.na.fill(fillDefaultValue)
# dim4.printSchema()
# display(dim4.orderBy(asc("a_uid")))
```
<details><summary> ì •ë‹µí™•ì¸</summary>

> uid ê°€ 4ë²ˆì¸ ì´ìš©ìë§Œ ë§¤ì¶œ ì •ë³´ê°€ 0 ì´ë©´ ì •ë‹µì…ë‹ˆë‹¤

</details>
<br>


### 7-7. ìƒì„±ëœ ìœ ì € í…Œì´ë¸”ì„ ì¬ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ì»¬ëŸ¼ ëª…ì„ ë³€ê²½í•©ë‹ˆë‹¤

* ì•„ë˜ì˜ ì¡°ê±´ì´ ë§Œì¡±í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”
  - ì§€í‘œì •ì˜ : ì»¬ëŸ¼ëª…ì´ `d_`ë¡œ ì‹œì‘í•˜ë„ë¡ ì¼ê´„ ë³€ê²½í•©ë‹ˆë‹¤
  - ì§€í‘œì‚°ì‹ : <kbd>withColumnRenamed("`a_uid`", "`d_uid`" )</kbd>
  - ì…ë ¥í˜•íƒœ : dim4
    - dimension í…Œì´ë¸”ì´ë¯€ë¡œ `d_`ë¡œ ì‹œì‘í•˜ëŠ” ì»¬ëŸ¼ ê·œì¹™ì„ ë”°ë¦…ë‹ˆë‹¤
    - access, purchase ì™€ ê°™ì´ ê°œë³„ í…Œì´ë¸”ì˜ prefix ë¥¼ ì´ìš©í•´ì„œ `d_a<column-name>` í˜¹ì€ `d_p<column-name>` ê·œì¹™ì„ ë”°ë¦…ë‹ˆë‹¤
  - ì¶œë ¥í˜•íƒœ : `d_uid`, `d_name`, `d_gender`, `d_acount`, `d_pamount`, `d_pcount`
  - ì •ë ¬í˜•íƒœ : `d_uid` asc

* 7-6 ì—ì„œ ìƒì„±í•œ dim4 ì˜ ëª¨ë“  ì»¬ëŸ¼ì´ `d_`ë¡œ ì‹œì‘í•˜ë„ë¡ Rename í•˜ì—¬ ì •ë¦¬í•©ë‹ˆë‹¤
```python
dim4.printSchema()
# dim5 = (
#     dim4
#     .withColumnRenamed("a_uid", "d_uid")
#     <ì»¬ëŸ¼ a_count ë¶€í„° u_gender ê¹Œì§€ d_ ë¡œ ì‹œì‘í•˜ë„ë¡ ì»¬ëŸ¼ëª… ë³€ê²½>
#    .drop("a_uid", "a_count", "p_amount", "p_count", "u_name", "u_gender")
#    .select("d_uid", "d_name", "d_gender", "d_acount", "d_pamount", "d_pcount")
# )
display(dim5.orderBy(asc("d_uid")))
```
<details><summary> ì •ë‹µí™•ì¸</summary>

> ëª¨ë“  ì»¬ëŸ¼ì´ `d_`ë¡œ ì‹œì‘í•˜ê³  6ê°œì˜ ì»¬ëŸ¼ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤ë©´ ì •ë‹µì…ë‹ˆë‹¤

</details>
<br>


### 7-8. ìµœì´ˆ êµ¬ë§¤ ìœ ì € ì •ë³´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤

* ì•„ë˜ì˜ ì¡°ê±´ì´ ë§Œì¡±í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”
  - ì§€í‘œì •ì˜ : ìµœì´ˆ êµ¬ë§¤ ì •ë³´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤ <kbd>expr("case when `d_first_purchase` is null then `p_time` else `d_first_purchase` end")</kbd>
  - ì§€í‘œì‚°ì‹ : í•´ë‹¹ ì¼ìì˜ ê°€ì¥ ì²˜ìŒ êµ¬ë§¤í•œ êµ¬ë§¤ì¼ì‹œë¥¼ ì´ìš©í•˜ì—¬ ìµœì´ˆêµ¬ë§¤ì¼ì„ êµ¬í•©ë‹ˆë‹¤ <kbd>dim5.withColumn("`d_first_purchase`, lit(None))</kbd>
  - ì…ë ¥í˜•íƒœ : dim5
  - ì¶œë ¥í˜•íƒœ : `d_uid`, `d_count`, `d_amount`, `d_count`, `d_name`, `d_gender`, `d_first_purchase`
  - ì •ë ¬í˜•íƒœ : `d_uid` asc

* selectFirstPurchaseTime ëŠ” í•˜ë£¨ì— ì—¬ëŸ¬ë²ˆ êµ¬ë§¤ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê°€ì¥ ë¨¼ì € êµ¬ë§¤í•œ ì¼ì‹œë¥¼ min í•¨ìˆ˜ë¥¼ `p_time` ì˜ ìµœì†Œê°’ì„ êµ¬í•©ë‹ˆë‹¤
```python
purchase.printSchema()
# selectFirstPurchaseTime = "select p_uid, <ìµœì†Œê°’í•¨ìˆ˜> as p_time from purchase <ì§‘ê³„êµ¬ë¬¸>"
# 
# first_purchase = spark.sql(selectFirstPurchaseTime)
# dim6 = dim5.withColumn("d_first_purchase", lit(None))
# dim6.printSchema()
# 
# exprFirstPurchase = expr("case when d_first_purchase is null then p_time else d_first_purchase end")
# 
# dim7 = (
#     dim6.join(first_purchase, dim5.d_uid == first_purchase.p_uid, "left_outer")
#     .withColumn("first_purchase", exprFirstPurchase)
#     .drop("d_first_purchase", "p_uid", "p_time")
#     .withColumnRenamed("first_purchase", "d_first_purchase")
# )
#     
# dimension = dim7.orderBy(asc("d_uid"))
# dimension.printSchema()
# display(dimension)
```
<details><summary> ì •ë‹µí™•ì¸</summary>

> 4ë²ˆ ê³ ê°ì˜ ì œì™¸í•œ ëª¨ë“  ì²« ë²ˆì§¸ êµ¬ë§¤ ì¼ì‹œê°€ ì¶œë ¥ë˜ë©´ ì •ë‹µì…ë‹ˆë‹¤

</details>
<br>


### 7-9. ìƒì„±ëœ ë””ë©˜ì ¼ì„ ì €ì¥ì†Œì— ì €ì¥í•©ë‹ˆë‹¤

* ì•„ë˜ì˜ ì¡°ê±´ì´ ë§Œì¡±í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”
  - ì €ì¥ìœ„ì¹˜ : "dimension/dt=20201025" <kbd>dataFrame.write</kbd>
  - ì €ì¥ì˜µì…˜ : ëŒ€ìƒ ê²½ë¡œê°€ ì¡´ì¬í•˜ë”ë¼ë„ ë®ì–´ì”ë‹ˆë‹¤ <kbd>.mode("overwrite")</kbd>
  - ì €ì¥í¬ë§· : íŒŒì¼€ì´  <kbd>.parquet("dimension/dt=20201025")</kbd>

```python
dimension.printSchema()
# target_dir="<ì €ì¥ê²½ë¡œ>"
# dimension.write.mode(<ì €ì¥ëª¨ë“œ>).parquet(target_dir)
```
<details><summary> ì •ë‹µí™•ì¸</summary>

> ì €ì¥ ì‹œì— ì˜¤ë¥˜ê°€ ì—†ê³  ëŒ€ìƒ ê²½ë¡œ(dimension/dt=20201025)ê°€ ìƒì„±ë˜ì—ˆë‹¤ë©´ ì„±ê³µì…ë‹ˆë‹¤

</details>
<br>


### 7-10. ìƒì„±ëœ ë””ë©˜ì ¼ì„ ë‹¤ì‹œ ì½ì–´ì„œ ì¶œë ¥í•©ë‹ˆë‹¤

* ì•„ë˜ì˜ ì¡°ê±´ì´ ë§Œì¡±í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”
  - ì €ì¥ìœ„ì¹˜ : "dimension/dt=20201025" <kbd>spark.read</kbd>
  - ì €ì¥í¬ë§· : íŒŒì¼€ì´  <kbd>.parquet("dimension/dt=20201025")</kbd>

```python
# newDimension = <ë””ë©˜ì ¼ì„ ì½ì–´ì˜µë‹ˆë‹¤>
# newDimension.printSchema()
# display(newDimension)
```
<details><summary> ì •ë‹µí™•ì¸</summary>

> ë””ë©˜ì ¼ í…Œì´ë¸”ì„ ì •ìƒì ìœ¼ë¡œ ì½ì–´ì™”ê³ , ë™ì¼í•œ ìŠ¤í‚¤ë§ˆì™€ ë°ì´í„°ê°€ ì¶œë ¥ë˜ì—ˆë‹¤ë©´ ì •ë‹µì…ë‹ˆë‹¤

</details>
<br>


## 8. :green_square: ì§ˆë¬¸ ë° ì»¨í…Œì´ë„ˆ ì¢…ë£Œ

### 8-1. ì§ˆë¬¸ê³¼ ë‹µë³€

### 8-2. ì»¨í…Œì´ë„ˆ ì¢…ë£Œ

```python
docker-compose down
```

> ì•„ë˜ì™€ ê°™ì€ ë©”ì‹œì§€ê°€ ì¶œë ¥ë˜ê³  ëª¨ë“  ì»¨í…Œì´ë„ˆê°€ ì¢…ë£Œë˜ë©´ ì •ìƒì…ë‹ˆë‹¤
```text
[+] Running 2/3
â ¿ Container fluentd   Removed        1.3s
â ¿ Container notebook  Removed        3.7s
â ¹ Container sqoop     Stopping       5.3s
```
<br>

